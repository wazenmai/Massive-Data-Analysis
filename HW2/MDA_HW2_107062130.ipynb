{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f93103f",
   "metadata": {},
   "source": [
    "# Report - 107062130 \n",
    "\n",
    "此專案以 MapReduce 的架構來實作 PageRank 的計算，除了輸入輸出，其餘都是用 pyspark 提供的 function 完成。\n",
    "- input: `input.txt`，每一行都是一條 link (\\<source\\> \\<destination\\>)\n",
    "\n",
    "\n",
    "- output: 前十個 PageRank 最高的點 (\\<node\\> \\<rank\\>)\n",
    "\n",
    "## 步驟說明\n",
    "### Preprocess\n",
    "1. `lines` : 把檔案讀進來用 `map` 變成 (\\<source\\>, \\<destination\\>) 的形式。\n",
    "\n",
    "\n",
    "2. 找出 node 總數 $N$: 利用 `map` 將 source 跟 destination 的點分開，再搭配 `union` 跟 `distinct` 可以得到所有的點 ID，最後用 `count` 算出 $N$。\n",
    "\n",
    "\n",
    "3. 找出 **lonelyNode** : 在測試自己生的小測資時發現沒有 in-link 的點會隨著計算慢慢消失，解法為用 `subtract` 找出那些只存在 source 卻不存在於 destination 的 node，每次計算時都加一條到接到自己、weight = 0 的 link。\n",
    "\n",
    "\n",
    "4. `links` : 把 `lines` 用 `map` 變成 (\\<source\\>, [\\<destination1\\, \\<destination2\\>...]) 的形式。\n",
    "\n",
    "\n",
    "5. `ranks` : 代表各個 node 的 PageRank 分數，一開始每個點的分數都是 $1/N$。\n",
    "\n",
    "### Computation （loop）\n",
    "將下面這些步驟重複計算 20 次\n",
    "1. `weights` : 算出經過一次計算後各個 source node 給 destination node 的 PageRank 值。\n",
    "    - `links.join(ranks)`: 得到 (\\<source\\>, ([\\<destination1\\, \\<destination2\\>...], \\<rank\\>))。\n",
    "    - `flatMap(lambda x: computeWeights(x[1][0], x[1][1]))` : 透過 `computeWeight` 計算 $\\beta$ * rank / len(destination)。\n",
    "  \n",
    "    \n",
    "2. `ranks`-v1: 用 `reduceByKey` 把 `weights` 中同個 node 的分數加總，再透過 `union` 加上 `lonelyNode` 的分數。\n",
    "\n",
    "\n",
    "3. $S$: 根據下方公式，$\\sum r'^{new}_j$ 就是目前 `ranks` 的分數加總，可直接用 `sum` 得到。\n",
    "    $$\\forall j:r^{new}_j=r'^{new}_j+\\frac{1-S}{N}  \\qquad \\text{where}:S=\\sum r'^{new}_j$$\n",
    " \n",
    " \n",
    "4. `ranks`-v2: 因為不會動到 key，所以直接用 `mapValues` 把 PageRank 的分數直接加上 $(1 - S)/N$。\n",
    "\n",
    "### Output\n",
    "計算完畢後，用 `sortBy` 讓 `ranks` 的分數可以由大到小排，取前十項用 `%f` 輸出即為答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3281b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16ad9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters\n",
    "beta = 0.8\n",
    "iteration = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059f0199",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/27 22:17:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/27 22:17:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10876\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"page-rank\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "lines = sc.textFile(\"input.txt\").map(lambda r: r.split('\\t'))\n",
    "\n",
    "# find N\n",
    "sourceNode = lines.map(lambda edge: edge[0])\n",
    "destNode = lines.map(lambda edge: edge[1])\n",
    "allNode = sourceNode.union(destNode).distinct().sortBy(lambda x: x)\n",
    "N = allNode.count()\n",
    "print(N)\n",
    "\n",
    "# fine lonelyNode\n",
    "lonelyNode = sourceNode.subtract(destNode).distinct().map(lambda x: (x, 0))\n",
    "print(lonelyNode.count())\n",
    "\n",
    "links = lines.map(lambda x: (x[0], x[1])).groupByKey()\n",
    "ranks = allNode.map(lambda x: (x, 1 / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c149aa09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4240 has rank: 0.0006321988095901939. -> 0.000632\n",
      "10861 has rank: 0.0006291557128603985. -> 0.000629\n",
      "6899 has rank: 0.0005239103397527083. -> 0.000524\n",
      "9526 has rank: 0.0005116224706020384. -> 0.000512\n",
      "2118 has rank: 0.0004956586476699695. -> 0.000496\n",
      "3419 has rank: 0.0004848441996390268. -> 0.000485\n",
      "1311 has rank: 0.00047961928931848386. -> 0.000480\n",
      "3186 has rank: 0.00047049755140743756. -> 0.000470\n",
      "3541 has rank: 0.00046289158656890174. -> 0.000463\n",
      "367 has rank: 0.00046151003829042697. -> 0.000462\n"
     ]
    }
   ],
   "source": [
    "def computeWeights(dest, rank):\n",
    "    num_dest = len(dest)\n",
    "    for d in dest:\n",
    "        yield (d, beta * (rank / num_dest))\n",
    "        \n",
    "for it in range(0, iteration):\n",
    "    weights = links.join(ranks).flatMap(lambda x: computeWeights(x[1][0], x[1][1]))\n",
    "    \n",
    "    ranks = weights.reduceByKey(lambda x, y: x + y).union(lonelyNode)\n",
    "    S = ranks.values().sum()\n",
    "    ranks = ranks.mapValues(lambda rank: rank + (1 - S) / N)\n",
    "\n",
    "ranks = ranks.sortBy(lambda x: x[1], ascending=False)\n",
    "rank = ranks.collect()\n",
    "\n",
    "with open(\"Outputfile.txt\", \"w\") as f:\n",
    "    for (link, pagerank) in rank[0:10]:\n",
    "        print(\"%s has rank: %s. -> %f\" % (link, pagerank, pagerank))\n",
    "        f.write(\"%d\\t%f\\n\" % (int(link), pagerank))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8da2b0",
   "metadata": {},
   "source": [
    "# Check Result (Please ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2785c1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4240\t0.000632\n",
      "\n",
      "10861\t0.000629\n",
      "\n",
      "6899\t0.000524\n",
      "\n",
      "9526\t0.000512\n",
      "\n",
      "2118\t0.000496\n",
      "\n",
      "3419\t0.000485\n",
      "\n",
      "1311\t0.000480\n",
      "\n",
      "3186\t0.000470\n",
      "\n",
      "3541\t0.000463\n",
      "\n",
      "367\t0.000462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"Outputfile.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfe724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
